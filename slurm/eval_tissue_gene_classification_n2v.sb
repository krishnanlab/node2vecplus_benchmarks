#!/bin/bash --login

#SBATCH -t 3:55:00
#SBATCH -N 1
#SBATCH -c 4
#SBATCH --mem=16GB
#SBATCH --array=0-139  # 28 networks, 5 q settings (5 p per job)
#SBATCH -o ../slurm_history/slurm-%A-%a.out

cd $SLURM_SUBMIT_DIR

networks=(
GTExCoExp-blood
GTExCoExp-blood_vessel
GTExCoExp-brain
GTExCoExp-global
GTExCoExp-heart
GTExCoExp-kidney
GTExCoExp-muscle
GTExCoExpTop-blood
GTExCoExpTop-blood_vessel
GTExCoExpTop-brain
GTExCoExpTop-global
GTExCoExpTop-heart
GTExCoExpTop-kidney
GTExCoExpTop-muscle
HumanBase-blood
HumanBase-blood_vessel
HumanBase-brain
HumanBase-global
HumanBase-heart
HumanBase-kidney
HumanBase-muscle
HumanBaseTop-blood
HumanBaseTop-blood_vessel
HumanBaseTop-brain
HumanBaseTop-global
HumanBaseTop-heart
HumanBaseTop-kidney
HumanBaseTop-muscle
)

pqs=(0.01 0.1 1 10 100)

ID=$SLURM_ARRAY_TASK_ID
network=${networks[$(expr $ID % 28)]}
q=${pqs[$(expr $ID / 28 / 9)]}

if [ $ID == 0 ]; then 
    # submit job for combining all results after all evaluation jobs are finished
    sbatch --dependency=afterany:$SLURM_ARRAY_JOB_ID combine_results.sb tissue_gene_classification_n2v
fi

cd ../script
conda activate node2vecplus-bench
for p in ${pqs[@]}; do
    time python eval_gene_classification_n2v.py --task tissue \
        --gene_universe HBGTX --network $network --p $p --q $q
done
